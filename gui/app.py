import streamlit as st
import requests
import numpy as np
import cv2
from PIL import Image
import pyttsx3
import threading
import time
import subprocess
import sys

API_URL = "http://127.0.0.1:8000/detect/image"

# =========================
# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
# =========================
st.set_page_config(
    page_title="AI Visual Assistant",
    layout="wide"
)

st.title("ğŸ¦¯ AI Visual Assistant Ù„Ù„Ù…ÙƒÙÙˆÙÙŠÙ†")
st.write("ÙƒØ´Ù Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ ÙˆØ§Ù„Ù…Ø³Ø§ÙØ© ÙˆØ§Ù„Ù†Ø·Ù‚ Ø§Ù„Ø¹Ø±Ø¨ÙŠ")

# =========================
# ğŸ”Š Ø¯Ø§Ù„Ø© Ø§Ù„ØµÙˆØª (Ù„ÙˆØ¶Ø¹ Ø§Ù„ØµÙˆØ±Ø© ÙÙ‚Ø·)
# =========================
def speak_ar(text):
    def run():
        engine = pyttsx3.init()
        engine.setProperty("rate", 140)

        for voice in engine.getProperty("voices"):
            if "arab" in voice.name.lower() or "ar" in voice.id.lower():
                engine.setProperty("voice", voice.id)
                break

        engine.say(text)
        engine.runAndWait()
        engine.stop()

    threading.Thread(target=run, daemon=True).start()

# =========================
# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù€ Cooldown
# =========================
COOLDOWN = 3

if "last_speech_time" not in st.session_state:
    st.session_state.last_speech_time = 0.0

# =========================
# Ø¯ÙˆØ§Ù„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ ÙˆØ§Ù„Ù…Ø³Ø§ÙØ©
# =========================
def get_direction(bbox, frame_width):
    x1, _, x2, _ = bbox
    center_x = (x1 + x2) / 2
    if center_x < frame_width / 3:
        return "Ø¹Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø±"
    elif center_x > 2 * frame_width / 3:
        return "Ø¹Ù„Ù‰ Ø§Ù„ÙŠÙ…ÙŠÙ†"
    else:
        return "ÙÙŠ Ø§Ù„Ù…Ù†ØªØµÙ"

def get_distance(bbox, frame_area):
    x1, y1, x2, y2 = bbox
    box_area = (x2 - x1) * (y2 - y1)
    ratio = box_area / frame_area
    if ratio > 0.20:
        return "Ù‚Ø±ÙŠØ¨ Ø¬Ø¯Ù‹Ø§"
    elif ratio > 0.08:
        return "Ù…ØªÙˆØ³Ø·"
    else:
        return "Ø¨Ø¹ÙŠØ¯"

# =========================
# ğŸ¥ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± (ÙƒØ§Ù…ÙŠØ±Ø§ Ø­Ù‚ÙŠÙ‚ÙŠØ©)
# =========================
st.markdown("---")
st.subheader("ğŸ¥ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± (ÙƒØ§Ù…ÙŠØ±Ø§ Ø­Ù‚ÙŠÙ‚ÙŠØ© + ØµÙˆØª)")

col_live1, col_live2 = st.columns(2)

# Ø²Ø± ØªØ´ØºÙŠÙ„
with col_live1:
    if st.button("â–¶ï¸ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©"):
        if "live_process" not in st.session_state or st.session_state.live_process is None:
            st.session_state.live_process = subprocess.Popen(
                [sys.executable, "gui/live_camera.py"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            st.success("âœ… ØªÙ… ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§")
        else:
            st.warning("âš ï¸ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ ØªØ¹Ù…Ù„ Ø¨Ø§Ù„ÙØ¹Ù„")

# Ø²Ø± Ø¥ÙŠÙ‚Ø§Ù
with col_live2:
    if st.button("â¹ï¸ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§"):
        if "live_process" in st.session_state and st.session_state.live_process is not None:
            st.session_state.live_process.terminate()
            st.session_state.live_process = None
            st.success("ğŸ›‘ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§")
        else:
            st.warning("âš ï¸ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ ØºÙŠØ± Ù…Ø´ØºÙ„Ø©")

# =========================
# ğŸ“· Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø£ÙˆÙ„: ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø©
# =========================
st.markdown("---")
st.subheader("ğŸ“· ÙƒØ´Ù ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø©")

camera_image = st.camera_input("Ø§Ù„ØªÙ‚Ø· ØµÙˆØ±Ø©")

if camera_image is not None:
    image = Image.open(camera_image).convert("RGB")
    image_np = np.array(image)

    h, w = image_np.shape[:2]
    frame_area = h * w

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©")
        st.image(image_np, use_column_width=True)

    if st.button("ğŸ” ÙƒØ´Ù Ø§Ù„Ø£Ø¬Ø³Ø§Ù…"):
        with st.spinner("â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„..."):
            response = requests.post(
                API_URL,
                files={
                    "file": (
                        "frame.jpg",
                        camera_image.getvalue(),
                        "image/jpeg"
                    )
                },
                timeout=30
            )

        if response.status_code == 200:
            data = response.json()
            detections = data["detections"]

            spoken_sentences = []

            for d in detections:
                bbox = d["bbox"]
                label = d["label"]

                direction = get_direction(bbox, w)
                distance = get_distance(bbox, frame_area)

                spoken_sentences.append(
                    f"ÙŠÙˆØ¬Ø¯ {label} {direction} ÙˆÙ‡Ùˆ {distance}"
                )

                x1, y1, x2, y2 = bbox
                cv2.rectangle(image_np, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.putText(
                    image_np,
                    f"{label} | {direction} | {distance}",
                    (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.6,
                    (0, 255, 0),
                    2
                )

            with col2:
                st.subheader("Ø§Ù„Ù†ØªÙŠØ¬Ø©")
                st.image(image_np, use_column_width=True)

            now = time.time()
            if spoken_sentences and (now - st.session_state.last_speech_time >= COOLDOWN):
                speak_ar("ØªÙ†Ø¨ÙŠÙ‡ØŒ " + " . ".join(spoken_sentences))
                st.session_state.last_speech_time = now

            st.subheader("ğŸ“Š Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ´Ù")
            st.json(data)

        else:
            st.error("âŒ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù€ API")
